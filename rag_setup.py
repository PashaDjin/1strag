"""
–°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ —Å Docling + HybridChunker.

–£–ª—É—á—à–µ–Ω–∏—è:
- HybridChunker: –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, token-aware
- Picture description: –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ö–µ–º –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
- Headings: –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
- JSON cache: –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ DoclingDocument
- contextualize(): –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ embedding
"""

import glob
import json
import os
from datetime import datetime
from pathlib import Path

from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.base_models import InputFormat
from docling.chunking import HybridChunker
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
DEFAULT_EMBED_MODEL = "intfloat/multilingual-e5-base"
DEFAULT_BOOKS_DIR = "books/"
DEFAULT_INDEX_DIR = "rag_index/"
DEFAULT_CACHE_DIR = "docling_cache/"
DEFAULT_MAX_TOKENS = 400  # –î–ª—è E5 (max 512, –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å)

# Picture description ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç VLM –º–æ–¥–µ–ª—å
ENABLE_PICTURE_DESCRIPTION = False  # –í–∫–ª—é—á–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ö–µ–º


def is_e5_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å E5."""
    return "e5" in model_name.lower()


class E5Embeddings:
    """
    Wrapper –¥–ª—è HuggingFaceEmbeddings —Å E5 –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏.
    
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç–æ–ª—å–∫–æ –ø—Ä–∏ embed, –ù–ï –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è —Ç–µ–∫—Å—Ç.
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.base = HuggingFaceEmbeddings(model_name=model_name)
        self._is_e5 = is_e5_model(model_name)
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        if self._is_e5:
            texts = [f"passage: {t}" for t in texts]
        return self.base.embed_documents(texts)
    
    def embed_query(self, text: str) -> list[float]:
        if self._is_e5:
            text = f"query: {text}"
        return self.base.embed_query(text)


def get_env_int(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç int –∏–∑ env."""
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def get_pdf_files(books_dir: str) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ PDF —Ñ–∞–π–ª–æ–≤."""
    pattern = os.path.join(books_dir, "*.pdf")
    return sorted(glob.glob(pattern))


def get_cache_path(pdf_path: str, cache_dir: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ JSON –∫–µ—à—É –¥–ª—è PDF."""
    pdf_name = Path(pdf_path).stem
    return os.path.join(cache_dir, f"{pdf_name}.json")


def load_cached_document(cache_path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DoclingDocument –∏–∑ JSON –∫–µ—à–∞."""
    from docling_core.types.doc.document import DoclingDocument as DLDocument
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return DLDocument.model_validate(data)
    except Exception as e:
        print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
        return None


def save_document_cache(doc, cache_path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DoclingDocument –≤ JSON –∫–µ—à."""
    os.makedirs(os.path.dirname(cache_path), exist_ok=True)
    
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(doc.export_to_dict(), f, ensure_ascii=False, indent=2)


def create_converter() -> DocumentConverter:
    """
    –°–æ–∑–¥–∞—ë—Ç DocumentConverter —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.
    """
    pipeline_options = PdfPipelineOptions()
    
    # OCR –æ—Ç–∫–ª—é—á–∞–µ–º ‚Äî –∫–Ω–∏–≥–∞ –ì–µ—Ä–∞—Å–∏–º–µ–Ω–∫–æ –Ω–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞
    pipeline_options.do_ocr = False
    
    # Picture description ‚Äî –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ö–µ–º
    if ENABLE_PICTURE_DESCRIPTION:
        try:
            from docling.datamodel.pipeline_options import smolvlm_picture_description
            pipeline_options.do_picture_description = True
            pipeline_options.picture_description_options = smolvlm_picture_description
            print("  üñºÔ∏è Picture description –≤–∫–ª—é—á–µ–Ω–æ (SmolVLM)")
        except ImportError:
            print("  ‚ö†Ô∏è SmolVLM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, picture description –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            pipeline_options.do_picture_description = False
    else:
        pipeline_options.do_picture_description = False
    
    return DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )


def convert_pdf_to_docling(pdf_path: str, converter: DocumentConverter, cache_dir: str):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ DoclingDocument.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–µ—à –µ—Å–ª–∏ –µ—Å—Ç—å.
    """
    cache_path = get_cache_path(pdf_path, cache_dir)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    cached_doc = load_cached_document(cache_path)
    if cached_doc is not None:
        print(f"  üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫–µ—à–∞: {cache_path}")
        return cached_doc
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
    print(f"  üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: {pdf_path}")
    result = converter.convert(pdf_path)
    doc = result.document
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    save_document_cache(doc, cache_path)
    print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫–µ—à: {cache_path}")
    
    return doc


def create_chunker(embed_model: str, max_tokens: int) -> HybridChunker:
    """
    –°–æ–∑–¥–∞—ë—Ç HybridChunker —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º embedding –º–æ–¥–µ–ª–∏.
    """
    print(f"  üîß –°–æ–∑–¥–∞–Ω–∏–µ HybridChunker (max_tokens={max_tokens}, merge_peers=False)")
    
    tokenizer = AutoTokenizer.from_pretrained(embed_model)
    
    return HybridChunker(
        tokenizer=tokenizer,
        max_tokens=max_tokens,
        merge_peers=False,  # –ú–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ ‚Äî –ª—É—á—à–µ –¥–ª—è retrieval
    )


def chunk_document(doc, chunker: HybridChunker, source_name: str) -> list[Document]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç DoclingDocument –Ω–∞ LangChain Documents.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç contextualize() –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è headings –≤ —Ç–µ–∫—Å—Ç.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç headings –≤ metadata.
    """
    chunks = list(chunker.chunk(dl_doc=doc))
    
    documents = []
    for idx, chunk in enumerate(chunks):
        # contextualize –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è embedding
        contextualized_text = chunker.contextualize(chunk)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º headings –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        headings = []
        if hasattr(chunk, 'meta') and chunk.meta:
            if hasattr(chunk.meta, 'headings') and chunk.meta.headings:
                headings = chunk.meta.headings
        
        # –°–æ–∑–¥–∞—ë–º LangChain Document
        doc = Document(
            page_content=contextualized_text,
            metadata={
                "source": source_name,
                "chunk_id": idx,
                "headings": headings,
                # –ü–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ "section" –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "section": headings[0] if headings else "",
            }
        )
        documents.append(doc)
    
    return documents


def save_chunks_for_debug(documents: list[Document], path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSONL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    with open(path, "w", encoding="utf-8") as f:
        for doc in documents:
            record = {
                "chunk_id": doc.metadata.get("chunk_id", 0),
                "source": doc.metadata.get("source", "unknown"),
                "section": doc.metadata.get("section", ""),
                "headings": doc.metadata.get("headings", []),
                "text": doc.page_content,
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def save_index_config(
    index_dir: str,
    embed_model: str,
    max_tokens: int,
    pdf_files: list[str],
    chunk_count: int,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç config.json."""
    config = {
        "embed_model": embed_model,
        "max_tokens": max_tokens,
        "merge_peers": False,
        "built_at": datetime.now().isoformat(),
        "pdf_count": len(pdf_files),
        "pdf_files": [os.path.basename(p) for p in pdf_files],
        "chunk_count": chunk_count,
        "chunker": "HybridChunker",
        "parser": "Docling",
        "picture_description": ENABLE_PICTURE_DESCRIPTION,
    }
    
    config_path = os.path.join(index_dir, "config.json")
    os.makedirs(index_dir, exist_ok=True)
    
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"  üìù –ö–æ–Ω—Ñ–∏–≥: {config_path}")


def build_index(documents: list[Document], index_dir: str, embed_model: str) -> None:
    """–°–æ–∑–¥–∞—ë—Ç FAISS –∏–Ω–¥–µ–∫—Å."""
    print(f"  üî¢ –°–æ–∑–¥–∞–Ω–∏–µ embeddings ({embed_model})...")
    
    embeddings = E5Embeddings(model_name=embed_model)
    
    print("  üì¶ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
    vectorstore = FAISS.from_documents(documents, embeddings)
    
    os.makedirs(index_dir, exist_ok=True)
    vectorstore.save_local(index_dir)
    print(f"  üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_dir}")


def rebuild_full_index(books_dir: str, index_dir: str) -> bool:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞.
    """
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ env
    embed_model = os.getenv("EMBED_MODEL", DEFAULT_EMBED_MODEL)
    cache_dir = os.getenv("CACHE_DIR", DEFAULT_CACHE_DIR)
    max_tokens = get_env_int("MAX_TOKENS", DEFAULT_MAX_TOKENS)
    debug_dump = os.getenv("DEBUG_DUMP_CHUNKS", "0") == "1"

    print("=" * 60)
    print("üöÄ –°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ (Docling + HybridChunker)")
    print("=" * 60)
    print(f"  üìÅ PDF –ø–∞–ø–∫–∞: {books_dir}")
    print(f"  üìÅ –ò–Ω–¥–µ–∫—Å: {index_dir}")
    print(f"  üìÅ –ö–µ—à: {cache_dir}")
    print(f"  üß† Embed model: {embed_model}")
    print(f"  üìè Max tokens: {max_tokens}")
    print(f"  üñºÔ∏è Picture description: {ENABLE_PICTURE_DESCRIPTION}")
    print()

    # 1. –ü–æ–ª—É—á–∞–µ–º PDF —Ñ–∞–π–ª—ã
    pdf_files = get_pdf_files(books_dir)
    if not pdf_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        return False

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ PDF: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"   ‚Ä¢ {os.path.basename(pdf)}")
    print()

    # 2. –°–æ–∑–¥–∞—ë–º –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –∏ —á–∞–Ω–∫–µ—Ä
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
    converter = create_converter()
    chunker = create_chunker(embed_model, max_tokens)
    print()

    # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ —á–∞–Ω–∫–∏–º –∫–∞–∂–¥—ã–π PDF
    all_documents = []
    
    for pdf_path in pdf_files:
        print(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞: {os.path.basename(pdf_path)}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è (—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        doc = convert_pdf_to_docling(pdf_path, converter, cache_dir)
        
        # Chunking
        source_name = os.path.basename(pdf_path)
        documents = chunk_document(doc, chunker, source_name)
        print(f"  ‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(documents)}")
        
        all_documents.extend(documents)
        print()

    print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_documents)}")
    print()

    # 4. Debug dump
    if debug_dump:
        chunks_path = "chunks.jsonl"
        print(f"üîç DEBUG: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {chunks_path}...")
        save_chunks_for_debug(all_documents, chunks_path)
        print()

    # 5. –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å
    print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    build_index(all_documents, index_dir, embed_model)
    print()

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    save_index_config(
        index_dir=index_dir,
        embed_model=embed_model,
        max_tokens=max_tokens,
        pdf_files=pdf_files,
        chunk_count=len(all_documents),
    )

    print()
    print("=" * 60)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print("=" * 60)
    return True


def main():
    """CLI —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    books_dir = os.getenv("BOOKS_DIR", DEFAULT_BOOKS_DIR)
    index_dir = os.getenv("INDEX_DIR", DEFAULT_INDEX_DIR)
    
    rebuild_full_index(books_dir, index_dir)


if __name__ == "__main__":
    main()
