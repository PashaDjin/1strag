"""
–°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞: –∑–∞–≥—Ä—É–∑–∫–∞ PDF, —á–∞–Ω–∫–∏–Ω–≥, —Å–æ–∑–¥–∞–Ω–∏–µ embeddings.

Stage 2 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ TECHNICAL_SPEC.md
"""

import glob
import json
import os
from datetime import datetime

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
DEFAULT_CHUNK_SIZE = 1500
DEFAULT_CHUNK_OVERLAP = 300
DEFAULT_EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
DEFAULT_BOOKS_DIR = "books/"
DEFAULT_INDEX_DIR = "rag_index/"
USE_SEMANTIC_CHUNKER = True  # –ù–æ–≤—ã–π —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥


def is_e5_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å E5 (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã query:/passage:)."""
    return "e5" in model_name.lower()


def add_passage_prefix(chunks: list, embed_model: str) -> list:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç 'passage: ' –ø—Ä–µ—Ñ–∏–∫—Å –∫ —Ç–µ–∫—Å—Ç—É —á–∞–Ω–∫–æ–≤ –¥–ª—è E5 –º–æ–¥–µ–ª–µ–π.
    E5 –æ–±—É—á–∞–ª—Å—è —Å —ç—Ç–∏–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ ‚Äî –±–µ–∑ –Ω–∏—Ö –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞–¥–∞–µ—Ç –Ω–∞ 10-20%.
    """
    if not is_e5_model(embed_model):
        return chunks
    
    print(f"  üìù –î–æ–±–∞–≤–ª—è–µ–º 'passage:' –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è E5 –º–æ–¥–µ–ª–∏")
    for chunk in chunks:
        chunk.page_content = f"passage: {chunk.page_content}"
    return chunks


def get_env_int(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç int –∏–∑ env."""
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {name}={value}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {default}")
        return default


def get_pdf_files(books_dir: str) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ PDF –≤ –ø–∞–ø–∫–µ books/."""
    pattern = os.path.join(books_dir, "*.pdf")
    return sorted(glob.glob(pattern))


def load_documents(pdf_paths: list[str]) -> list:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ —á–µ—Ä–µ–∑ PyPDFLoader(mode="page").
    –ö–∞–∂–¥—ã–π Document —Å–æ–¥–µ—Ä–∂–∏—Ç metadata: {source, page}.
    """
    all_pages = []
    for pdf_path in pdf_paths:
        print(f"  üìÑ –ó–∞–≥—Ä—É–∑–∫–∞: {pdf_path}")
        loader = PyPDFLoader(pdf_path, mode="page")
        pages = loader.load()
        all_pages.extend(pages)
    return all_pages


def split_documents(docs: list, chunk_size: int, chunk_overlap: int, embed_model: str = None) -> list:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏.
    
    –ï—Å–ª–∏ USE_SEMANTIC_CHUNKER=True:
      –ò—Å–ø–æ–ª—å–∑—É–µ—Ç SemanticChunker ‚Äî –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ —Å–º—ã—Å–ª—É —á–µ—Ä–µ–∑ embeddings.
      –°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ—Å—Ç–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ, —Ç–∞–±–ª–∏—Ü—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—é—Ç—Å—è.
    
    –ò–Ω–∞—á–µ:
      RecursiveCharacterTextSplitter ‚Äî —Ä–µ–∂–µ—Ç –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∏–º–≤–æ–ª–æ–≤.
    """
    if USE_SEMANTIC_CHUNKER and embed_model:
        print(f"  üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º SemanticChunker (—É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –ø–æ —Å–º—ã—Å–ª—É)")
        embeddings = HuggingFaceEmbeddings(model_name=embed_model)
        splitter = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type="percentile",  # –∏–ª–∏ "standard_deviation"
            breakpoint_threshold_amount=70,  # —á–µ–º –≤—ã—à–µ, —Ç–µ–º –∫—Ä—É–ø–Ω–µ–µ —á–∞–Ω–∫–∏
        )
        # SemanticChunker —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–µ–∫—Å—Ç–æ–º, –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        all_chunks = []
        for doc in docs:
            chunks = splitter.create_documents(
                [doc.page_content],
                metadatas=[doc.metadata]
            )
            all_chunks.extend(chunks)
        return all_chunks
    else:
        print(f"  ‚úÇÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º RecursiveCharacterTextSplitter")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
        )
        return splitter.split_documents(docs)


def save_chunks_for_debug(chunks: list, path: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSONL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
    –í–ù–ò–ú–ê–ù–ò–ï: –§–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ PDF (–∫–æ–ø–∏—Ä–∞–π—Ç). 
    –ù–µ –∫–æ–º–º–∏—Ç–∏—Ç—å –≤ git!
    """
    with open(path, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(chunks):
            record = {
                "chunk_id": i,
                "source": chunk.metadata.get("source", "unknown"),
                "page": chunk.metadata.get("page", 0),
                "text": chunk.page_content,
            }
            # –î–æ–±–∞–≤–ª—è–µ–º page_label –µ—Å–ª–∏ –µ—Å—Ç—å
            if "page_label" in chunk.metadata:
                record["page_label"] = chunk.metadata["page_label"]
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def save_index_config(
    index_dir: str,
    chunk_size: int,
    chunk_overlap: int,
    embed_model: str,
    pdf_files: list[str],
    chunk_count: int,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç config.json —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞."""
    config = {
        "embed_model": embed_model,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "built_at": datetime.now().isoformat(),
        "pdf_count": len(pdf_files),
        "pdf_files": [os.path.basename(p) for p in pdf_files],
        "chunk_count": chunk_count,
    }
    config_path = os.path.join(index_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"  üìù –ö–æ–Ω—Ñ–∏–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {config_path}")


def build_index(chunks: list, index_dir: str, embed_model: str) -> None:
    """–°–æ–∑–¥–∞—ë—Ç embeddings –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å."""
    # –î–æ–±–∞–≤–ª—è–µ–º passage: –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è E5 –º–æ–¥–µ–ª–µ–π
    chunks = add_passage_prefix(chunks, embed_model)
    
    print(f"  üî¢ –°–æ–∑–¥–∞–Ω–∏–µ embeddings ({embed_model})...")
    embeddings = HuggingFaceEmbeddings(model_name=embed_model)
    
    print("  üì¶ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(index_dir, exist_ok=True)
    
    vectorstore.save_local(index_dir)
    print(f"  üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_dir}")


def rebuild_full_index(books_dir: str, index_dir: str) -> bool:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –≤—ã–∑—ã–≤–∞–µ—Ç –≤—Å–µ –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –Ω–µ—Ç PDF.
    """
    # –ß–∏—Ç–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ env
    chunk_size = get_env_int("CHUNK_SIZE", DEFAULT_CHUNK_SIZE)
    chunk_overlap = get_env_int("CHUNK_OVERLAP", DEFAULT_CHUNK_OVERLAP)
    embed_model = os.getenv("EMBED_MODEL", DEFAULT_EMBED_MODEL)
    debug_dump = os.getenv("DEBUG_DUMP_CHUNKS", "0") == "1"

    print("=" * 50)
    print("üöÄ –°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞")
    print("=" * 50)
    print(f"  üìÅ –ü–∞–ø–∫–∞ PDF: {books_dir}")
    print(f"  üìÅ –ü–∞–ø–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {index_dir}")
    print(f"  üìè chunk_size: {chunk_size}")
    print(f"  üìè chunk_overlap: {chunk_overlap}")
    print(f"  üß† embed_model: {embed_model}")
    print()

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ PDF
    pdf_files = get_pdf_files(books_dir)
    if not pdf_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        print("   –ü–æ–ª–æ–∂–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞.")
        return False

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ PDF: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"   ‚Ä¢ {pdf}")
    print()

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü...")
    pages = load_documents(pdf_files)
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")
    print()

    # 3. –ß–∞–Ω–∫–∏–Ω–≥
    print("‚úÇÔ∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = split_documents(pages, chunk_size, chunk_overlap, embed_model)
    print(f"   –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    print()

    # 4. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–∞–º–ø —á–∞–Ω–∫–æ–≤
    if debug_dump:
        chunks_path = "chunks.jsonl"
        print(f"üîç DEBUG: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ {chunks_path}...")
        save_chunks_for_debug(chunks, chunks_path)
        print(f"   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: —Ñ–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ø–∏—Ä–∞–π—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç!")
        print()

    # 5. –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    build_index(chunks, index_dir, embed_model)
    print()

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    save_index_config(
        index_dir=index_dir,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        embed_model=embed_model,
        pdf_files=pdf_files,
        chunk_count=len(chunks),
    )

    print()
    print("=" * 50)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print("=" * 50)
    return True


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    books_dir = os.getenv("BOOKS_DIR", DEFAULT_BOOKS_DIR)
    index_dir = os.getenv("INDEX_DIR", DEFAULT_INDEX_DIR)
    
    rebuild_full_index(books_dir, index_dir)


if __name__ == "__main__":
    main()
