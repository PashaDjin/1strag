"""
–°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞: Docling + HybridChunker.

HybridChunker:
- –ù–ï —Ä–∞–∑—Ä—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã
- –ü–æ–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏ ‚Üí —á–∞–Ω–∫–∏)
- Token-aware (–Ω–µ —Ä–µ–∂–µ—Ç –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ)
- –î–æ–±–∞–≤–ª—è–µ—Ç headings –≤ metadata
- –¢–∞–±–ª–∏—Ü—ã –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ (MarkdownTableSerializer)
"""

import glob
import json
import os
from datetime import datetime
from pathlib import Path

from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.hierarchical_chunker import (
    ChunkingDocSerializer,
    ChunkingSerializerProvider,
)
from docling_core.transforms.serializer.markdown import MarkdownTableSerializer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
DEFAULT_MAX_TOKENS = 500  # –¢–æ–∫–µ–Ω—ã! E5-base –ª–∏–º–∏—Ç 512, –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å
DEFAULT_EMBED_MODEL = "intfloat/multilingual-e5-base"
DEFAULT_BOOKS_DIR = "books/"
DEFAULT_INDEX_DIR = "rag_index/"
CACHE_DIR = ".docling_cache"  # –ö–µ—à DoclingDocument –≤ JSON


class MarkdownTableSerializerProvider(ChunkingSerializerProvider):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π provider –¥–ª—è HybridChunker.
    –¢–∞–±–ª–∏—Ü—ã —Å–µ—Ä–∏–∞–ª–∏–∑—É—é—Ç—Å—è –≤ Markdown —Ñ–æ—Ä–º–∞—Ç –≤–º–µ—Å—Ç–æ triplet (Key=Value).
    """
    def get_serializer(self, doc):
        return ChunkingDocSerializer(
            doc=doc,
            table_serializer=MarkdownTableSerializer(),
        )


def is_e5_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å E5 (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã query:/passage:)."""
    return "e5" in model_name.lower()


class E5Embeddings:
    """
    Wrapper –¥–ª—è HuggingFaceEmbeddings —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ E5 –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏.
    
    E5 –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç:
    - 'passage: ' –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    - 'query: ' –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
    
    –≠—Ç–æ—Ç wrapper –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –ù–ï –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è page_content.
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.base = HuggingFaceEmbeddings(model_name=model_name)
        self._is_e5 = is_e5_model(model_name)
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Embed –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å 'passage:' –ø—Ä–µ—Ñ–∏–∫—Å–æ–º."""
        if self._is_e5:
            texts = [f"passage: {t}" for t in texts]
        return self.base.embed_documents(texts)
    
    def embed_query(self, text: str) -> list[float]:
        """Embed –∑–∞–ø—Ä–æ—Å–∞ —Å 'query:' –ø—Ä–µ—Ñ–∏–∫—Å–æ–º."""
        if self._is_e5:
            text = f"query: {text}"
        return self.base.embed_query(text)


def get_env_int(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç int –∏–∑ env."""
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {name}={value}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {default}")
        return default


def get_pdf_files(books_dir: str) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ PDF –≤ –ø–∞–ø–∫–µ books/."""
    pattern = os.path.join(books_dir, "*.pdf")
    return sorted(glob.glob(pattern))


def get_cache_path(pdf_path: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ JSON –∫–µ—à—É –¥–ª—è PDF."""
    os.makedirs(CACHE_DIR, exist_ok=True)
    pdf_name = Path(pdf_path).stem
    return os.path.join(CACHE_DIR, f"{pdf_name}.json")


def load_docling_document(pdf_path: str, converter: DocumentConverter):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç DoclingDocument –∏–∑ –∫–µ—à–∞ –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF.
    
    –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—Ç ~2-3 –º–∏–Ω—É—Ç—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö —Å–±–æ—Ä–∫–∞—Ö.
    """
    cache_path = get_cache_path(pdf_path)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    if os.path.exists(cache_path):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ PDF –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
        pdf_mtime = os.path.getmtime(pdf_path)
        cache_mtime = os.path.getmtime(cache_path)
        
        if cache_mtime > pdf_mtime:
            print(f"    üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–µ—à–∞: {cache_path}")
            try:
                from docling_core.types import DoclingDocument as DoclingDoc
                with open(cache_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                return DoclingDoc.model_validate(data)
            except Exception as e:
                print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–Ω–æ–≤–æ")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF
    print(f"    üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF...")
    result = converter.convert(pdf_path)
    doc = result.document
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    try:
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(doc.export_to_dict(), f, ensure_ascii=False)
        print(f"    üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫–µ—à: {cache_path}")
    except Exception as e:
        print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–µ—à: {e}")
    
    return doc


def chunk_with_hybrid_chunker(
    docling_doc,
    pdf_path: str,
    max_tokens: int,
) -> list[Document]:
    """
    –ß–∞–Ω–∫–∏—Ç DoclingDocument —á–µ—Ä–µ–∑ HybridChunker.
    
    HybridChunker:
    - –ü–æ–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
    - –ù–ï —Ä–∞–∑—Ä—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
    - Token-aware (—É—á–∏—Ç—ã–≤–∞–µ—Ç max_tokens)
    - –î–æ–±–∞–≤–ª—è–µ—Ç headings –≤ metadata
    - –¢–∞–±–ª–∏—Ü—ã –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ (–Ω–µ Key=Value –∫–∞—à–∞)
    
    contextualize() –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ embedding.
    """
    chunker = HybridChunker(
        max_tokens=max_tokens,
        merge_peers=True,  # –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏
        serializer_provider=MarkdownTableSerializerProvider(),  # –¢–∞–±–ª–∏—Ü—ã –≤ Markdown!
    )
    
    chunks = list(chunker.chunk(dl_doc=docling_doc))
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ LangChain Documents
    documents = []
    for i, chunk in enumerate(chunks):
        # –ü–æ–ª—É—á–∞–µ–º headings –¥–ª—è section
        headings = []
        if hasattr(chunk, 'meta') and chunk.meta:
            if hasattr(chunk.meta, 'headings') and chunk.meta.headings:
                headings = chunk.meta.headings
        
        section = " > ".join(headings) if headings else f"–ß–∞–Ω–∫ {i+1}"
        
        # contextualize() –¥–æ–±–∞–≤–ª—è–µ—Ç headings –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ embedding
        # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç E5 –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞
        try:
            enriched_text = chunker.contextualize(chunk)
        except Exception:
            enriched_text = chunk.text
        
        doc = Document(
            page_content=enriched_text,
            metadata={
                "source": pdf_path,
                "section": section,
                "headings": headings,
                "chunk_id": i,
            }
        )
        documents.append(doc)
    
    return documents


def save_chunks_for_debug(chunks: list[Document], path: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSONL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
    –í–ù–ò–ú–ê–ù–ò–ï: –§–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ PDF (–∫–æ–ø–∏—Ä–∞–π—Ç). 
    –ù–µ –∫–æ–º–º–∏—Ç–∏—Ç—å –≤ git!
    """
    with open(path, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(chunks):
            record = {
                "chunk_id": i,
                "source": chunk.metadata.get("source", "unknown"),
                "section": chunk.metadata.get("section", ""),
                "headings": chunk.metadata.get("headings", []),
                "text": chunk.page_content,
                "text_len": len(chunk.page_content),
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def save_index_config(
    index_dir: str,
    max_tokens: int,
    embed_model: str,
    pdf_files: list[str],
    chunk_count: int,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç config.json —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞."""
    config = {
        "embed_model": embed_model,
        "chunker": "HybridChunker",
        "max_tokens": max_tokens,
        "built_at": datetime.now().isoformat(),
        "pdf_count": len(pdf_files),
        "pdf_files": [os.path.basename(p) for p in pdf_files],
        "chunk_count": chunk_count,
    }
    config_path = os.path.join(index_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"  üìù –ö–æ–Ω—Ñ–∏–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {config_path}")


def build_index(chunks: list[Document], index_dir: str, embed_model: str) -> None:
    """–°–æ–∑–¥–∞—ë—Ç embeddings –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å."""
    print(f"  üî¢ –°–æ–∑–¥–∞–Ω–∏–µ embeddings ({embed_model})...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º E5Embeddings wrapper ‚Äî –æ–Ω —Å–∞–º –¥–æ–±–∞–≤–ª—è–µ—Ç prefix –ø—Ä–∏ embed
    embeddings = E5Embeddings(model_name=embed_model)
    
    print("  üì¶ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(index_dir, exist_ok=True)
    
    vectorstore.save_local(index_dir)
    print(f"  üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_dir}")


def rebuild_full_index(books_dir: str, index_dir: str) -> bool:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞.
    
    Pipeline:
    1. –ù–∞–π—Ç–∏ PDF —Ñ–∞–π–ª—ã
    2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ Docling (—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    3. –ß–∞–Ω–∫–∏–Ω–≥ —á–µ—Ä–µ–∑ HybridChunker
    4. –°–æ–∑–¥–∞—Ç—å embeddings (E5 —Å prefix)
    5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å
    """
    # –ß–∏—Ç–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ env
    max_tokens = get_env_int("MAX_TOKENS", DEFAULT_MAX_TOKENS)
    embed_model = os.getenv("EMBED_MODEL", DEFAULT_EMBED_MODEL)
    debug_dump = os.getenv("DEBUG_DUMP_CHUNKS", "0") == "1"

    print("=" * 50)
    print("üöÄ –°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ (Docling + HybridChunker)")
    print("=" * 50)
    print(f"  üìÅ –ü–∞–ø–∫–∞ PDF: {books_dir}")
    print(f"  üìÅ –ü–∞–ø–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {index_dir}")
    print(f"  üìè max_tokens: {max_tokens}")
    print(f"  üß† embed_model: {embed_model}")
    print(f"  üîß chunker: HybridChunker")
    print()

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ PDF
    pdf_files = get_pdf_files(books_dir)
    if not pdf_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        print("   –ü–æ–ª–æ–∂–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞.")
        return False

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ PDF: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"   ‚Ä¢ {pdf}")
    print()

    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ —á–∞–Ω–∫–∞–µ–º
    converter = DocumentConverter()
    all_chunks = []
    
    for pdf_path in pdf_files:
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path}")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º DoclingDocument (–∏–∑ –∫–µ—à–∞ –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º)
            docling_doc = load_docling_document(pdf_path, converter)
            
            # –ß–∞–Ω–∫–∏–Ω–≥ —á–µ—Ä–µ–∑ HybridChunker
            chunks = chunk_with_hybrid_chunker(docling_doc, pdf_path, max_tokens)
            all_chunks.extend(chunks)
            
            print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            continue
    
    print()
    print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    print()

    if not all_chunks:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")
        return False

    # 3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–∞–º–ø —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    if debug_dump:
        chunks_path = "chunks.jsonl"
        print(f"üîç DEBUG: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ {chunks_path}...")
        save_chunks_for_debug(all_chunks, chunks_path)
        print(f"   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: —Ñ–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ø–∏—Ä–∞–π—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç!")
        print()

    # 4. –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    build_index(all_chunks, index_dir, embed_model)
    print()

    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    save_index_config(
        index_dir=index_dir,
        max_tokens=max_tokens,
        embed_model=embed_model,
        pdf_files=pdf_files,
        chunk_count=len(all_chunks),
    )

    print()
    print("=" * 50)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print("=" * 50)
    return True


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    books_dir = os.getenv("BOOKS_DIR", DEFAULT_BOOKS_DIR)
    index_dir = os.getenv("INDEX_DIR", DEFAULT_INDEX_DIR)
    
    rebuild_full_index(books_dir, index_dir)


if __name__ == "__main__":
    main()
