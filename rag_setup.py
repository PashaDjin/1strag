"""
–°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞: –∑–∞–≥—Ä—É–∑–∫–∞ PDF —á–µ—Ä–µ–∑ Docling, —á–∞–Ω–∫–∏–Ω–≥, —Å–æ–∑–¥–∞–Ω–∏–µ embeddings.

Docling –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–∞–±–ª–∏—Ü!
"""

import glob
import json
import os
from datetime import datetime

from docling.document_converter import DocumentConverter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import MarkdownTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
DEFAULT_CHUNK_SIZE = 1500
DEFAULT_CHUNK_OVERLAP = 300
DEFAULT_EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
DEFAULT_BOOKS_DIR = "books/"
DEFAULT_INDEX_DIR = "rag_index/"

# Docling –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –≤ Markdown ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º MarkdownTextSplitter
USE_MARKDOWN_SPLITTER = True


def is_e5_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å E5 (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã query:/passage:)."""
    return "e5" in model_name.lower()


class E5Embeddings:
    """
    Wrapper –¥–ª—è HuggingFaceEmbeddings —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ E5 –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏.
    
    E5 –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç:
    - 'passage: ' –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    - 'query: ' –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
    
    –≠—Ç–æ—Ç wrapper –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –ù–ï –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç.
    –¢–µ–∫—Å—Ç –≤ page_content –æ—Å—Ç–∞—ë—Ç—Å—è —á–∏—Å—Ç—ã–º!
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.base = HuggingFaceEmbeddings(model_name=model_name)
        self._is_e5 = is_e5_model(model_name)
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Embed –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å 'passage:' –ø—Ä–µ—Ñ–∏–∫—Å–æ–º."""
        if self._is_e5:
            texts = [f"passage: {t}" for t in texts]
        return self.base.embed_documents(texts)
    
    def embed_query(self, text: str) -> list[float]:
        """Embed –∑–∞–ø—Ä–æ—Å–∞ —Å 'query:' –ø—Ä–µ—Ñ–∏–∫—Å–æ–º."""
        if self._is_e5:
            text = f"query: {text}"
        return self.base.embed_query(text)


def get_env_int(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç int –∏–∑ env."""
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {name}={value}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {default}")
        return default


def get_pdf_files(books_dir: str) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ PDF –≤ –ø–∞–ø–∫–µ books/."""
    pattern = os.path.join(books_dir, "*.pdf")
    return sorted(glob.glob(pattern))


def load_documents_with_docling(pdf_paths: list[str]) -> list[Document]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —á–µ—Ä–µ–∑ Docling –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ Markdown.
    
    Docling:
    - –†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏)
    - –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –≤ Markdown —Ç–∞–±–ª–∏—Ü—ã
    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ LangChain Document —Å page_content –≤ Markdown.
    """
    converter = DocumentConverter()
    all_docs = []
    
    for pdf_path in pdf_paths:
        print(f"  üìÑ Docling –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç: {pdf_path}")
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            result = converter.convert(pdf_path)
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown (—Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞–Ω—É—Ç Markdown —Ç–∞–±–ª–∏—Ü–∞–º–∏!)
            markdown_content = result.document.export_to_markdown()
            
            # –°–æ–∑–¥–∞—ë–º LangChain Document
            doc = Document(
                page_content=markdown_content,
                metadata={
                    "source": pdf_path,
                    "page": 0,  # Docling –Ω–µ –¥–∞—ë—Ç –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—É—é —Ä–∞–∑–±–∏–≤–∫—É
                    "format": "markdown",
                    "converter": "docling",
                }
            )
            all_docs.append(doc)
            
            print(f"    ‚úÖ –°–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(markdown_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {pdf_path}: {e}")
            continue
    
    return all_docs


def split_documents(docs: list, chunk_size: int, chunk_overlap: int) -> list:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏.
    
    –î–ª—è Markdown (Docling) –∏—Å–ø–æ–ª—å–∑—É–µ–º MarkdownTextSplitter:
    - –†–µ–∂–µ—Ç –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (# ## ###)
    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü
    
    –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ‚Äî RecursiveCharacterTextSplitter.
    """
    if USE_MARKDOWN_SPLITTER:
        print(f"  üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º MarkdownTextSplitter (–¥–ª—è Docling)")
        splitter = MarkdownTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
    else:
        print(f"  ‚úÇÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º RecursiveCharacterTextSplitter")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
        )
    
    return splitter.split_documents(docs)


def save_chunks_for_debug(chunks: list, path: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSONL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
    –í–ù–ò–ú–ê–ù–ò–ï: –§–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ PDF (–∫–æ–ø–∏—Ä–∞–π—Ç). 
    –ù–µ –∫–æ–º–º–∏—Ç–∏—Ç—å –≤ git!
    """
    with open(path, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(chunks):
            record = {
                "chunk_id": i,
                "source": chunk.metadata.get("source", "unknown"),
                "page": chunk.metadata.get("page", 0),
                "text": chunk.page_content,
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def save_index_config(
    index_dir: str,
    chunk_size: int,
    chunk_overlap: int,
    embed_model: str,
    pdf_files: list[str],
    chunk_count: int,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç config.json —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞."""
    config = {
        "embed_model": embed_model,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "built_at": datetime.now().isoformat(),
        "pdf_count": len(pdf_files),
        "pdf_files": [os.path.basename(p) for p in pdf_files],
        "chunk_count": chunk_count,
        "pdf_parser": "docling",
        "markdown_splitter": USE_MARKDOWN_SPLITTER,
    }
    config_path = os.path.join(index_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"  üìù –ö–æ–Ω—Ñ–∏–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {config_path}")


def build_index(chunks: list, index_dir: str, embed_model: str) -> None:
    """
    –°–æ–∑–¥–∞—ë—Ç embeddings –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å.
    
    –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º E5Embeddings wrapper –∫–æ—Ç–æ—Ä—ã–π –¥–æ–±–∞–≤–ª—è–µ—Ç prefix
    —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ embedding, –ù–ï –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è page_content!
    """
    print(f"  üî¢ –°–æ–∑–¥–∞–Ω–∏–µ embeddings ({embed_model})...")
    
    # E5Embeddings –¥–æ–±–∞–≤–ª—è–µ—Ç 'passage:' –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ embed_documents
    embeddings = E5Embeddings(model_name=embed_model)
    
    print("  üì¶ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(index_dir, exist_ok=True)
    
    vectorstore.save_local(index_dir)
    print(f"  üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_dir}")


def rebuild_full_index(books_dir: str, index_dir: str) -> bool:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –≤—ã–∑—ã–≤–∞–µ—Ç –≤—Å–µ –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –Ω–µ—Ç PDF.
    """
    # –ß–∏—Ç–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ env
    chunk_size = get_env_int("CHUNK_SIZE", DEFAULT_CHUNK_SIZE)
    chunk_overlap = get_env_int("CHUNK_OVERLAP", DEFAULT_CHUNK_OVERLAP)
    embed_model = os.getenv("EMBED_MODEL", DEFAULT_EMBED_MODEL)
    debug_dump = os.getenv("DEBUG_DUMP_CHUNKS", "0") == "1"

    print("=" * 50)
    print("üöÄ –°–±–æ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ (Docling + Markdown)")
    print("=" * 50)
    print(f"  üìÅ –ü–∞–ø–∫–∞ PDF: {books_dir}")
    print(f"  üìÅ –ü–∞–ø–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {index_dir}")
    print(f"  üìè chunk_size: {chunk_size}")
    print(f"  üìè chunk_overlap: {chunk_overlap}")
    print(f"  üß† embed_model: {embed_model}")
    print(f"  üìÑ PDF –ø–∞—Ä—Å–µ—Ä: Docling")
    print()

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ PDF
    pdf_files = get_pdf_files(books_dir)
    if not pdf_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        print("   –ü–æ–ª–æ–∂–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞.")
        return False

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ PDF: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"   ‚Ä¢ {pdf}")
    print()

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ—Ä–µ–∑ Docling (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown)
    print("üìñ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF —á–µ—Ä–µ–∑ Docling...")
    docs = load_documents_with_docling(pdf_files)
    if not docs:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        return False
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    print()

    # 3. –ß–∞–Ω–∫–∏–Ω–≥
    print("‚úÇÔ∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = split_documents(docs, chunk_size, chunk_overlap)
    print(f"   –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    print()

    # 4. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–∞–º–ø —á–∞–Ω–∫–æ–≤
    if debug_dump:
        chunks_path = "chunks.jsonl"
        print(f"üîç DEBUG: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ {chunks_path}...")
        save_chunks_for_debug(chunks, chunks_path)
        print(f"   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: —Ñ–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ø–∏—Ä–∞–π—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç!")
        print()

    # 5. –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    build_index(chunks, index_dir, embed_model)
    print()

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    save_index_config(
        index_dir=index_dir,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        embed_model=embed_model,
        pdf_files=pdf_files,
        chunk_count=len(chunks),
    )

    print()
    print("=" * 50)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print("=" * 50)
    return True


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    books_dir = os.getenv("BOOKS_DIR", DEFAULT_BOOKS_DIR)
    index_dir = os.getenv("INDEX_DIR", DEFAULT_INDEX_DIR)
    
    rebuild_full_index(books_dir, index_dir)


if __name__ == "__main__":
    main()
