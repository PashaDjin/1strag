"""
Streamlit UI + RAG-—á–∞—Ç —Å retriever + llm –Ω–∞–ø—Ä—è–º—É—é.

Stage 3: Core RAG logic (—Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑ UI).
Stage 4: main() + Streamlit UI.
"""

import json
import os

import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
DEFAULT_INDEX_DIR = "rag_index/"
DEFAULT_TOP_K = 20  # –£–≤–µ–ª–∏—á–µ–Ω–æ: —á–∞–Ω–∫–∏ 500 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
DEFAULT_OLLAMA_MODEL = "qwen2.5:14b"  # –õ—É—á—à–∏–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: qwen2.5:7b

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
SYSTEM_PROMPT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, –≥–æ—Ç–æ–≤—è—â–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –∫—É—Ä—Å–∞.

–Ø–ó–´–ö: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ, –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏–ª–∏ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤.

–ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏ "–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —ç—Ç–æ –Ω–µ –æ–ø–∏—Å–∞–Ω–æ".

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
1. –ú–∏–Ω–∏–º—É–º 3-5 –∞–±–∑–∞—Ü–µ–≤ ‚Äî –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã
2. –ü–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º –º—ã—Å–ª–µ–Ω–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ‚Üí –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ ‚Üí –ø—Ä–∏–º–µ—Ä—ã ‚Üí –∏—Ç–æ–≥
4. –†–∞—Å–∫—Ä–æ–π –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç: —á—Ç–æ —ç—Ç–æ, –∑–∞—á–µ–º –Ω—É–∂–Ω–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç
5. –ü—Ä–∏–≤–æ–¥–∏ —Ü–∏—Ç–∞—Ç—ã –∏ —Ñ–æ—Ä–º—É–ª—ã –¥–æ—Å–ª–æ–≤–Ω–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
6. –ï—Å–ª–∏ –µ—Å—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è ‚Äî –æ–±—ä—è—Å–Ω–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É –Ω–∏–º–∏
7. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ –¥–ª—è –Ω–µ—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞

–§–û–†–ú–ê–¢:
- –¢–µ—Ä–º–∏–Ω—ã –≤—ã–¥–µ–ª—è–π **–∂–∏—Ä–Ω—ã–º**
- –î–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π –Ω—É–º–µ—Ä–∞—Ü–∏—é
- –§–æ—Ä–º—É–ª—ã: ROE = –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å / –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª

–ö–æ–Ω—Ç–µ–∫—Å—Ç ({chunk_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤):
{context}

–í–æ–ø—Ä–æ—Å: {question}

–†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç (–Ω–∞ —Ä—É—Å—Å–∫–æ–º):"""


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è env ---

def get_env_int(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç int –∏–∑ env."""
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞ ---

def load_index_config(index_dir: str) -> dict | None:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç config.json —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–±–æ—Ä–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç.
    """
    config_path = os.path.join(index_dir, "config.json")
    if not os.path.exists(config_path):
        return None
    
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ embed_model ‚Äî –æ—Å—Ç–∞–ª—å–Ω–æ–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    if "embed_model" not in config:
        raise RuntimeError(
            "‚ùå –í config.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'embed_model'.\n"
            "   –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ –∏–Ω–¥–µ–∫—Å: python rag_setup.py"
        )
    return config


def is_e5_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å E5 (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã query:/passage:)."""
    return "e5" in model_name.lower()


class E5QueryEmbeddings(HuggingFaceEmbeddings):
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ HuggingFaceEmbeddings, –¥–æ–±–∞–≤–ª—è—é—â–∞—è 'query: ' –ø—Ä–µ—Ñ–∏–∫—Å.
    E5 –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ ‚Äî —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞!
    """
    def embed_query(self, text: str) -> list[float]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç query: –ø—Ä–µ—Ñ–∏–∫—Å –ø–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞."""
        return super().embed_query(f"query: {text}")


@st.cache_resource
def load_index(index_dir: str, embed_model: str):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å —Å –¥–∏—Å–∫–∞.
    Embeddings —Å–æ–∑–¥–∞—é—Ç—Å—è –ó–î–ï–°–¨ –û–î–ò–ù –†–ê–ó.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ—Ç.
    
    –í–ù–ò–ú–ê–ù–ò–ï: allow_dangerous_deserialization=True –±–µ–∑–æ–ø–∞—Å–Ω–æ
    –¢–û–õ–¨–ö–û –¥–ª—è –≤–∞—à–µ–≥–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞. –ù–µ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —á—É–∂–∏–µ –∏–Ω–¥–µ–∫—Å—ã!
    """
    index_path = os.path.join(index_dir, "index.faiss")
    if not os.path.exists(index_path):
        return None
    
    # –î–ª—è E5 –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ë—Ä—Ç–∫—É —Å query: –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
    if is_e5_model(embed_model):
        embeddings = E5QueryEmbeddings(model_name=embed_model)
    else:
        embeddings = HuggingFaceEmbeddings(model_name=embed_model)
    
    vectorstore = FAISS.load_local(
        index_dir,
        embeddings,
        allow_dangerous_deserialization=True  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–≤–æ–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞!
    )
    return vectorstore


def get_retriever(vectorstore, top_k: int):
    """
    –°–æ–∑–¥–∞—ë—Ç retriever —Å –∑–∞–¥–∞–Ω–Ω—ã–º k.
    –í–ê–ñ–ù–û: k –∑–∞–¥–∞—ë—Ç—Å—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ retriever, –ù–ï –ø—Ä–∏ –≤—ã–∑–æ–≤–µ.
    """
    return vectorstore.as_retriever(search_kwargs={"k": top_k})


# --- LLM ---

@st.cache_resource
def get_llm(model: str) -> OllamaLLM:
    """–°–æ–∑–¥–∞—ë—Ç LLM –∏–∑ Ollama."""
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    return OllamaLLM(
        base_url=base_url,
        model=model,
        temperature=0,
    )


def check_ollama_connection(llm: OllamaLLM) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π GET –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ invoke –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.
    """
    import urllib.request
    import urllib.error
    
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    try:
        req = urllib.request.Request(f"{base_url}/api/tags", method="GET")
        with urllib.request.urlopen(req, timeout=5) as response:
            return response.status == 200
    except Exception:
        return False


# --- –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø—Ä–æ–º–ø—Ç ---

def format_context(docs: list) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π "X –∏–∑ Y".
    –ü–æ—Ä—è–¥–æ–∫: –º–µ–Ω–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞, —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π ‚Äî –≤ –∫–æ–Ω—Ü–µ.
    (LLM –ª—É—á—à–µ –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç –∫–æ–Ω–µ—Ü –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî "recency bias")
    """
    if not docs:
        return ""
    
    total = len(docs)
    reversed_docs = list(reversed(docs))
    
    fragments = []
    for i, doc in enumerate(reversed_docs, 1):
        section = doc.metadata.get("section", "?")
        header = f"[–§—Ä–∞–≥–º–µ–Ω—Ç {i} –∏–∑ {total}, {section}]"
        fragments.append(f"{header}\n{doc.page_content}")
    
    return "\n\n---\n\n".join(fragments)


def build_prompt(context: str, question: str, chunk_count: int) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM."""
    return SYSTEM_PROMPT.format(
        context=context, 
        question=question,
        chunk_count=chunk_count
    )


# --- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ---

def format_sources(docs: list) -> list[str]:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è.
    """
    seen = set()
    result = []
    for doc in docs:
        filename = os.path.basename(doc.metadata.get("source", "unknown"))
        section = doc.metadata.get("section", "")
        source = f"{filename} [{section}]" if section else filename
        if source not in seen:
            seen.add(source)
            result.append(source)
    return result



# --- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ ---

def ask_question(retriever, llm, question: str) -> tuple[str, list]:
    """
    –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è retriever –∏ llm –Ω–∞–ø—Ä—è–º—É—é.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (answer, docs).
    """
    # 1. –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    docs = retriever.invoke(question)
    
    # 2. –ï—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –ù–ï –≤—ã–∑—ã–≤–∞–µ–º LLM
    if not docs:
        return "–í –∫–Ω–∏–≥–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.", []
    
    # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
    context = format_context(docs)
    prompt = build_prompt(context, question, len(docs))
    answer = llm.invoke(prompt)
    
    return answer, docs


# --- Stage 4: Streamlit UI ---

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    st.set_page_config(
        page_title="RAG —á–∞—Ç–±–æ—Ç –ø–æ PDF",
        page_icon="üìö",
        layout="wide",
    )
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    st.title("üìö RAG —á–∞—Ç–±–æ—Ç –ø–æ PDF")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∏–∑ env
    index_dir = os.getenv("INDEX_DIR", DEFAULT_INDEX_DIR)
    books_dir = os.getenv("BOOKS_DIR", "books/")
    top_k = get_env_int("TOP_K", DEFAULT_TOP_K)
    ollama_model = os.getenv("OLLAMA_MODEL", DEFAULT_OLLAMA_MODEL)
    
    # --- Sidebar ---
    with st.sidebar:
        st.header("‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        
        # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
        
        st.divider()
        
        # –ö–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
        if st.button("üîÑ –ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å", use_container_width=True):
            with st.spinner("–ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞..."):
                # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å circular import
                from rag_setup import rebuild_full_index
                
                success = rebuild_full_index(books_dir, index_dir)
                
                if success:
                    st.cache_resource.clear()
                    st.success("‚úÖ –ò–Ω–¥–µ–∫—Å –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω!")
                    st.rerun()
                else:
                    st.error(f"‚ùå –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        
        st.divider()
        
        # –°—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞
        st.subheader("üìä –°—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞")
        config = load_index_config(index_dir)
        
        if config:
            st.success("‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω")
            st.caption(f"**PDF:** {config.get('pdf_count', '?')}")
            st.caption(f"**–ß–∞–Ω–∫–æ–≤:** {config.get('chunk_count', '?')}")
            st.caption(f"**Chunker:** {config.get('chunker', 'legacy')}")
            st.caption(f"**Max tokens:** {config.get('max_tokens', config.get('chunk_size', '?'))}")
        else:
            st.warning("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
            st.caption("–ü–æ–ª–æ–∂–∏—Ç–µ PDF –≤ books/ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å'")
    
    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏–Ω–¥–µ–∫—Å–∞ ---
    if not config:
        st.warning(
            "üìÇ **–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω.**\n\n"
            "1. –ü–æ–ª–æ–∂–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É `books/`\n"
            "2. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É **–ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å** –≤ sidebar\n\n"
            "–ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ: `python rag_setup.py`"
        )
        st.stop()
    
    # --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ ---
    try:
        vectorstore = load_index(index_dir, config["embed_model"])
        if not vectorstore:
            st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ –µ–≥–æ.")
            st.stop()
        
        retriever = get_retriever(vectorstore, top_k)
        llm = get_llm(ollama_model)
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        st.stop()
    
    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama ---
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ –∏–ª–∏ –µ—Å–ª–∏ —Ñ–ª–∞–≥ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if "ollama_checked" not in st.session_state:
        with st.spinner("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama..."):
            if not check_ollama_connection(llm):
                st.error(
                    "‚ùå **Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.**\n\n"
                    "–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å:\n"
                    "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: `ollama serve`\n"
                    f"2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: `ollama pull {ollama_model}`"
                )
                st.stop()
            st.session_state.ollama_checked = True
    
    # --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ ---
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            if message.get("sources"):
                with st.expander("üìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏"):
                    for source in message["sources"]:
                        st.caption(f"‚Ä¢ {source}")
    
    # --- –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
    if user_input := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–Ω–∏–≥–∞–º..."):
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
        })
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        with st.chat_message("assistant"):
            with st.spinner("–î—É–º–∞—é..."):
                try:
                    answer, docs = ask_question(retriever, llm, user_input)
                    sources = format_sources(docs)
                    
                except RuntimeError as e:
                    answer = f"‚ùå –û—à–∏–±–∫–∞: {e}"
                    sources = []
                except Exception as e:
                    answer = f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}"
                    sources = []
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            st.markdown(answer)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if sources:
                with st.expander("üìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏", expanded=True):
                    for source in sources:
                        st.caption(f"‚Ä¢ {source}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources,
        })


if __name__ == "__main__":
    main()
